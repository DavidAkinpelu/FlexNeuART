{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural model: vanilla BERT ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to move to the top-level directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training requires exporting data in the format (with a slight modification) of the \n",
    "CEDR framework ([MacAvaney et al' 2019](https://github.com/Georgetown-IR-Lab/cedr)).\n",
    "\n",
    "The following command\n",
    "generates training data in the CEDR format for the collection `wikipedia_dpr_nq_sample`\n",
    "and the field `text_raw`. The traing data is generated from the split `bitext`, \n",
    "whereas split `dev` is used to generate validation data. During export, one can generate negatives of three types: hard (top-K entries), medium (top-K sample), and easy (sampled from the whole collection). Typically, hard and easy negatives are not particularly useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: collections\n",
      "========================================================\n",
      "Train split: bitext\n",
      "Eval split: dev\n",
      "Random seed: 0\n",
      "Output directory: collections/wikipedia_dpr_nq_sample/derived_data/cedr_train/text_raw/\n",
      "# of threads: 4\n",
      "Index export field: text_raw\n",
      "Query export field: text_raw\n",
      "Candidate provider parameters:  -cand_prov \"lucene\" -u \"collections/wikipedia_dpr_nq_sample/lucene_index\"  -cand_prov_add_conf \"collections/wikipedia_dpr_nq_sample/exper_desc.best/lucene.json\"\n",
      "Resource parameters: -fwd_index_dir \"collections/wikipedia_dpr_nq_sample/forward_index/\" -embed_dir \"collections/wikipedia_dpr_nq_sample/derived_data/embeddings/\" -giza_root_dir \"collections/wikipedia_dpr_nq_sample/derived_data/giza\" \n",
      "A # of hard/medium/easy samples per query: 0/3/0\n",
      "A max. # of candidate records to generate training data: 50\n",
      "A max. # of candidate records to generate test data: 50\n",
      "Max train query # param.: \n",
      "Max test/dev query # param.:  -max_num_query_test 5000 \n",
      "Case handling param: \n",
      "========================================================\n",
      "JAVA_OPTS=-Xms16469316k -Xmx28821303k -server\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportTrainPairs - Candidate provider type: lucene URI: collections/wikipedia_dpr_nq_sample/lucene_index config: collections/wikipedia_dpr_nq_sample/exper_desc.best/lucene.json\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportTrainPairs - Number of threads: 4\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.cand_providers.CandidateProvider - Provider type: lucene\n",
      "URI: collections/wikipedia_dpr_nq_sample/lucene_index\n",
      "Config file: collections/wikipedia_dpr_nq_sample/exper_desc.best/lucene.json\n",
      "# of threads: 4\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Lucene candidate provider k1=0.800000, b=0.300000 query field name: text Exact field match?: false\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.ForwardIndexBinaryMapDb - Finished loading context from file: collections/wikipedia_dpr_nq_sample/forward_index//text_raw.mapdb\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportTrainNegSampleBase - # of hard/medium/easy samples per query: 0/3/0\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.utils.RandomUtils - New random generator with seed: 0\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportTrainNegSampleBase - Lower-casing? true\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportTrainNegSampleBase - # top-scoring training candidates to sample/select from 50\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportTrainNegSampleBase - # top-scoring training candidates to select positives from 50 (only for export with scores)\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportTrainNegSampleBase - # top candidates for validation 50\n",
      "[Thread-5] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 1279\n",
      "[Thread-4] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 1390\n",
      "[Thread-3] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 3157\n",
      "[Thread-5] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 4691\n",
      "[Thread-4] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 13414\n",
      "[Thread-4] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 21190\n",
      "[Thread-3] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 30873\n",
      "[Thread-2] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 33336\n",
      "[Thread-5] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 33943\n",
      "[Thread-3] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 37961\n",
      "[Thread-5] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 49231\n",
      "[Thread-3] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 49873\n",
      "[Thread-2] WARN edu.cmu.lti.oaqa.flexneuart.cand_providers.LuceneCandidateProvider - Ignoring empty query #: 53608\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportTrainCEDR - Generated data for 56380 queries 416352 documents 712179 training query-doc pairs\n"
     ]
    }
   ],
   "source": [
    "!scripts/export_train/export_cedr.sh \\\n",
    "  wikipedia_dpr_nq_sample \\\n",
    "  text_raw \\\n",
    "  bitext \\\n",
    "  dev \\\n",
    "  -out_subdir cedr_train/text_raw \\\n",
    "  -cand_train_qty 50 \\\n",
    "  -cand_test_qty 50 \\\n",
    "  -thread_qty 4 \\\n",
    "  -hard_neg_qty 0 \\\n",
    "  -sample_easy_neg_qty 0 \\\n",
    "  -sample_med_neg_qty 3 \\\n",
    "  -max_num_query_test 5000 \\\n",
    "  -cand_prov lucene \\\n",
    "  -cand_prov_add_conf exper_desc.best/lucene.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we can use a wrapper convenience script that reads most parameters from a configuration file. \n",
    "\n",
    "Note that the following ``train_model.sh`` scripts assumes that the training data path is **relative** to the ``derived_data`` subdirectory while other paths are **relative** to the collection root. The training script has a number of options (check them out by running with the option ``-h``). Here is how one can run a training script (remember this requires a GPU and pytorch with CUDA support). By default the script validates after each epoch, but this behavior can be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/cedr/train_model.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    cedr_train/text_raw \\\n",
    "     vanilla_bert \\\n",
    "     -seed 0 \\\n",
    "     -add_exper_subdir todays_experiment \\\n",
    "     -json_conf  model_conf/vanilla_bert.json \\\n",
    "     -epoch_qty 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts runs, both training and evaluation. The respective statistics is stored in a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"0\": {\r\n",
      "        \"loss\": 0.3480607439222474,\r\n",
      "        \"score\": 0.6714521193722517,\r\n",
      "        \"lr\": 0.0002,\r\n",
      "        \"bert_lr\": 2e-05,\r\n",
      "        \"train_time\": 4923.056172847748,\r\n",
      "        \"validation_time\": 1545.6733376979828\r\n",
      "    },\r\n",
      "    \"1\": {\r\n",
      "        \"loss\": 0.25530940150896,\r\n",
      "        \"score\": 0.672424202490772,\r\n",
      "        \"lr\": 0.00019,\r\n",
      "        \"bert_lr\": 1.9e-05,\r\n",
      "        \"train_time\": 4912.885874032974,\r\n",
      "        \"validation_time\": 1548.907298564911\r\n",
      "    },\r\n",
      "    \"2\": {\r\n",
      "        \"loss\": 0.22298829897231495,\r\n",
      "        \"score\": 0.6784664800502341,\r\n",
      "        \"lr\": 0.0001805,\r\n",
      "        \"bert_lr\": 1.805e-05,\r\n",
      "        \"train_time\": 4920.855644226074,\r\n",
      "        \"validation_time\": 1547.028527021408\r\n",
      "    },\r\n",
      "    \"3\": {\r\n",
      "        \"loss\": 0.19422185843908618,\r\n",
      "        \"score\": 0.6758941456344162,\r\n",
      "        \"lr\": 0.00017147499999999998,\r\n",
      "        \"bert_lr\": 1.71475e-05,\r\n",
      "        \"train_time\": 4920.853113651276,\r\n",
      "        \"validation_time\": 1546.0641107559204\r\n",
      "    }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat collections/wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/todays_experiment/base/0/train_stat.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is possible to train a neural model in a fusion mode.\n",
    "\n",
    "Here, we optimize for the neural model score fused with the score of a candidate generator. This requires knowing a good weight for the candidate generator score. \n",
    "Here, we assum that the score 1.0 is good enough and export data as shown in the next cell. Please, note the parameter `cand_train_4pos_qty`, which controls the depth of the pool from which we select positive examples. We normally want this pool to be larger than the pool from which we select negative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/export_train/export_cedr_with_scores.sh \\\n",
    "  wikipedia_dpr_nq_sample \\\n",
    "  text_raw \\\n",
    "  bitext \\\n",
    "  dev \\\n",
    "  -out_subdir cedr_train_with_scores/text_raw \\\n",
    "  -cand_train_qty 50 \\\n",
    "  -cand_test_qty 50 \\\n",
    "  -cand_train_4pos_qty 1000 \\\n",
    "  -thread_qty 4 \\\n",
    "  -hard_neg_qty 0 \\\n",
    "  -sample_easy_neg_qty 0 \\\n",
    "  -sample_med_neg_qty 3 \\\n",
    "  -max_num_query_test 5000 \\\n",
    "  -cand_prov lucene \\\n",
    "  -cand_prov_add_conf exper_desc.best/lucene.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importantly__ to train a model we:\n",
    "1. Use a different configuration file (`model_conf/vanilla_bert_with_scores.json`) that sets candidate provider weights to be non-zero.\n",
    "2. Newly generated training data that exports scores (`cedr_train_with_scores/text_raw`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/cedr/train_model.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    cedr_train_with_scores/text_raw \\\n",
    "     vanilla_bert \\\n",
    "     -seed 0 \\\n",
    "     -add_exper_subdir todays_experiment_with_scores \\\n",
    "     -json_conf  model_conf/vanilla_bert_with_scores.json \\\n",
    "     -epoch_qty 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and testing statistics can be found in this JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"0\": {\r\n",
      "        \"loss\": 0.3928542813032928,\r\n",
      "        \"score\": 0.672083845882564,\r\n",
      "        \"lr\": 0.0002,\r\n",
      "        \"bert_lr\": 2e-05,\r\n",
      "        \"train_time\": 4668.770467042923,\r\n",
      "        \"validation_time\": 1543.5844690799713\r\n",
      "    },\r\n",
      "    \"1\": {\r\n",
      "        \"loss\": 0.2927950551674534,\r\n",
      "        \"score\": 0.6839066897667362,\r\n",
      "        \"lr\": 0.00019,\r\n",
      "        \"bert_lr\": 1.9e-05,\r\n",
      "        \"train_time\": 4698.543385028839,\r\n",
      "        \"validation_time\": 1547.0706918239594\r\n",
      "    },\r\n",
      "    \"2\": {\r\n",
      "        \"loss\": 0.2543279381992271,\r\n",
      "        \"score\": 0.6847354303138471,\r\n",
      "        \"lr\": 0.0001805,\r\n",
      "        \"bert_lr\": 1.805e-05,\r\n",
      "        \"train_time\": 4704.753187894821,\r\n",
      "        \"validation_time\": 1544.000947713852\r\n",
      "    },\r\n",
      "    \"3\": {\r\n",
      "        \"loss\": 0.2291002025931528,\r\n",
      "        \"score\": 0.68495924843781,\r\n",
      "        \"lr\": 0.00017147499999999998,\r\n",
      "        \"bert_lr\": 1.71475e-05,\r\n",
      "        \"train_time\": 4703.747788667679,\r\n",
      "        \"validation_time\": 1542.5429928302765\r\n",
      "    }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat collections/wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/todays_experiment_with_scores/base/0/train_stat.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
