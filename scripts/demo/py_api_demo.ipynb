{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to move to the top-level directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leo/SourceTreeGit/FlexNeuART.refact2021\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION='wikipedia_dpr_nq_sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.py_flexneuart.setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Java JAR to the class path\n",
    "configure_classpath('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a resource manager\n",
    "resource_manager=create_featextr_resource_manager(resource_root_dir=f'collections/{COLLECTION}/',\n",
    "                                                  fwd_index_dir='forward_index',\n",
    "                                                  model1_root_dir=f'derived_data/giza',\n",
    "                                                  embed_root_dir=f'derived_data/embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.config import QUESTION_FILE_JSON, QREL_FILE, DOCID_FIELD, TEXT_FIELD_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.py_flexneuart.cand_provider import *\n",
    "# create a candidate provider/generator\n",
    "cand_prov = create_cand_provider(resource_manager, PROVIDER_TYPE_LUCENE, f'lucene_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TEXT = \"vein carry blood heart away\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1329,\n",
       " [CandidateEntry(doc_id='639661', score=18.328275680541992),\n",
       "  CandidateEntry(doc_id='472789', score=16.816619873046875),\n",
       "  CandidateEntry(doc_id='1776205', score=16.630727767944336),\n",
       "  CandidateEntry(doc_id='639669', score=15.6367826461792),\n",
       "  CandidateEntry(doc_id='8448903', score=15.448601722717285),\n",
       "  CandidateEntry(doc_id='8448902', score=15.369601249694824),\n",
       "  CandidateEntry(doc_id='639670', score=15.27547550201416),\n",
       "  CandidateEntry(doc_id='639663', score=14.904623985290527),\n",
       "  CandidateEntry(doc_id='35722', score=14.59425163269043),\n",
       "  CandidateEntry(doc_id='1302853', score=14.318553924560547),\n",
       "  CandidateEntry(doc_id='639671', score=14.157160758972168),\n",
       "  CandidateEntry(doc_id='1786523', score=14.077558517456055),\n",
       "  CandidateEntry(doc_id='588394', score=13.997241973876953),\n",
       "  CandidateEntry(doc_id='639690', score=13.810718536376953),\n",
       "  CandidateEntry(doc_id='1450640', score=13.643953323364258),\n",
       "  CandidateEntry(doc_id='3936360', score=13.642525672912598),\n",
       "  CandidateEntry(doc_id='5622935', score=13.536111831665039),\n",
       "  CandidateEntry(doc_id='2992576', score=13.500545501708984),\n",
       "  CandidateEntry(doc_id='47133', score=13.166474342346191),\n",
       "  CandidateEntry(doc_id='47129', score=13.15163803100586)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of running a text query\n",
    "query_res = run_text_query(cand_prov, 20, QUERY_TEXT)\n",
    "query_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1329,\n",
       " [CandidateEntry(doc_id='639661', score=18.328275680541992),\n",
       "  CandidateEntry(doc_id='472789', score=16.816619873046875),\n",
       "  CandidateEntry(doc_id='1776205', score=16.630727767944336),\n",
       "  CandidateEntry(doc_id='639669', score=15.6367826461792),\n",
       "  CandidateEntry(doc_id='8448903', score=15.448601722717285),\n",
       "  CandidateEntry(doc_id='8448902', score=15.369601249694824),\n",
       "  CandidateEntry(doc_id='639670', score=15.27547550201416),\n",
       "  CandidateEntry(doc_id='639663', score=14.904623985290527),\n",
       "  CandidateEntry(doc_id='35722', score=14.59425163269043),\n",
       "  CandidateEntry(doc_id='1302853', score=14.318553924560547),\n",
       "  CandidateEntry(doc_id='639671', score=14.157160758972168),\n",
       "  CandidateEntry(doc_id='1786523', score=14.077558517456055),\n",
       "  CandidateEntry(doc_id='588394', score=13.997241973876953),\n",
       "  CandidateEntry(doc_id='639690', score=13.810718536376953),\n",
       "  CandidateEntry(doc_id='1450640', score=13.643953323364258),\n",
       "  CandidateEntry(doc_id='3936360', score=13.642525672912598),\n",
       "  CandidateEntry(doc_id='5622935', score=13.536111831665039),\n",
       "  CandidateEntry(doc_id='2992576', score=13.500545501708984),\n",
       "  CandidateEntry(doc_id='47133', score=13.166474342346191),\n",
       "  CandidateEntry(doc_id='47129', score=13.15163803100586)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of running a generic query interface\n",
    "query_res = run_query(cand_prov, 20, {TEXT_FIELD_NAME : QUERY_TEXT}, default_query_id=FAKE_QUERY_ID)\n",
    "query_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1329,\n",
       " [CandidateEntry(doc_id='639661', score=18.328275680541992),\n",
       "  CandidateEntry(doc_id='472789', score=16.816619873046875),\n",
       "  CandidateEntry(doc_id='1776205', score=16.630727767944336),\n",
       "  CandidateEntry(doc_id='639669', score=15.6367826461792),\n",
       "  CandidateEntry(doc_id='8448903', score=15.448601722717285),\n",
       "  CandidateEntry(doc_id='8448902', score=15.369601249694824),\n",
       "  CandidateEntry(doc_id='639670', score=15.27547550201416),\n",
       "  CandidateEntry(doc_id='639663', score=14.904623985290527),\n",
       "  CandidateEntry(doc_id='35722', score=14.59425163269043),\n",
       "  CandidateEntry(doc_id='1302853', score=14.318553924560547),\n",
       "  CandidateEntry(doc_id='639671', score=14.157160758972168),\n",
       "  CandidateEntry(doc_id='1786523', score=14.077558517456055),\n",
       "  CandidateEntry(doc_id='588394', score=13.997241973876953),\n",
       "  CandidateEntry(doc_id='639690', score=13.810718536376953),\n",
       "  CandidateEntry(doc_id='1450640', score=13.643953323364258),\n",
       "  CandidateEntry(doc_id='3936360', score=13.642525672912598),\n",
       "  CandidateEntry(doc_id='5622935', score=13.536111831665039),\n",
       "  CandidateEntry(doc_id='2992576', score=13.500545501708984),\n",
       "  CandidateEntry(doc_id='47133', score=13.166474342346191),\n",
       "  CandidateEntry(doc_id='47129', score=13.15163803100586)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of running a generic query interface\n",
    "query_res = run_query(cand_prov, 20, {DOCID_FIELD: FAKE_QUERY_ID, TEXT_FIELD_NAME : QUERY_TEXT})\n",
    "query_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward index demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.py_flexneuart.fwd_index import get_forward_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's play with a raw index that keeps ony unparsed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_indx = get_forward_index(resource_manager, 'text_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'textRaw'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index type\n",
    "raw_indx.indx_fld_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vein \"Vein Veins are blood vessels that carry blood toward the heart. Most veins carry deoxygenated blood from the tissues back to the heart; exceptions are the pulmonary and umbilical veins, both of which carry oxygenated blood to the heart. In contrast to veins, arteries carry blood away from the heart. Veins are less muscular than arteries and are often closer to the skin. There are valves in most veins to prevent backflow. Veins are present throughout the body as tubes that carry blood back to the heart. Veins are classified in a number of ways, including superficial vs. deep, pulmonary\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_indx.get_doc_text_raw('639661')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A parsed index has more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_indx = get_forward_index(resource_manager, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parsedText'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index type\n",
    "parsed_indx.indx_fld_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocEntryParsed(word_ids=[75, 144, 210, 246, 506, 587, 589, 591, 594, 867, 1268, 1282, 2311, 2516, 3125, 3352, 4121, 5121, 7795, 8410, 8455, 12461, 14717, 14722, 14724, 23655, 23669, 27261, 59794, 102036], word_qtys=[1, 1, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 9, 6, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1], word_id_seq=[7795, 7795, 102036, 8410, 5121, 506, 8410, 210, 7795, 506, 23655, 8410, 587, 2311, 210, 14724, 23669, 7795, 506, 14717, 8410, 210, 4121, 7795, 8455, 506, 8410, 3125, 210, 7795, 27261, 8455, 246, 589, 14722, 7795, 1282, 59794, 7795, 1268, 75, 2516, 506, 8410, 2311, 210, 7795, 3352, 144, 867, 591, 12461, 594, 14724], doc_len=54)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_indx.get_doc_parsed('639661')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('body', WordEntry(word_id=75, word_freq=17735))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's extract the first document word and its info\n",
    "parsed_indx.get_word_by_id(75), parsed_indx.get_word_entry_by_id(75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranker API demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.py_flexneuart.ranker import *\n",
    "from scripts.py_flexneuart.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model files and feature extractor configuration is relative to the collection (resource root) directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE_NAME='exper_desc.best/models/bm25_model1.model'\n",
    "FEAT_EXTR_FILE_NAME='exper_desc.best/extractors/bm25=text+model1=text_bert_tok+lambda=0.3+probSelfTran=0.35.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, we load queries using a full path or relative path that includes collection directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FILE_NAME=f'collections/{COLLECTION}/input_data/dev/{QUESTION_FILE_JSON}'\n",
    "QREL_FILE_NAME=f'collections/{COLLECTION}/input_data/dev/{QREL_FILE}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A toy example where we generate a list of candidates for merely one query (using the candidate provider) and re-rank them using the Java-layer re-ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_ranker_bm25_model1 = JavaQueryRanker(resource_manager, \n",
    "                              feat_extr_file_name=FEAT_EXTR_FILE_NAME, \n",
    "                              model_file_name=MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('639661', 0.4589444396983704),\n",
       " ('1776205', 0.4341061334562677),\n",
       " ('472789', 0.4324617381414335),\n",
       " ('8448903', 0.4124776432310974),\n",
       " ('8448902', 0.41143788988374125),\n",
       " ('35722', 0.39951390916561724),\n",
       " ('639669', 0.38760818984443224),\n",
       " ('3936360', 0.3864522671342978),\n",
       " ('1450640', 0.38388484121055044),\n",
       " ('588394', 0.38289955583934543),\n",
       " ('639670', 0.3823782759630887),\n",
       " ('639663', 0.3775039059463895),\n",
       " ('47133', 0.3731257557936975),\n",
       " ('1302853', 0.36679545440286726),\n",
       " ('639690', 0.3653150559189846),\n",
       " ('639671', 0.36361852471115813),\n",
       " ('1786523', 0.36268165971062455),\n",
       " ('2992576', 0.353630291595394),\n",
       " ('5622935', 0.3518175514979561),\n",
       " ('47129', 0.34765838000119853)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_dict = {DOCID_FIELD : FAKE_QUERY_ID, \n",
    "              TEXT_FIELD_NAME : QUERY_TEXT}\n",
    "java_ranker_bm25_model1.rank_candidates(query_res[1], query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There's a function (used only for evaluation) to score candidates without sorting them scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'639661': 0.4589444396983704,\n",
       " '472789': 0.4324617381414335,\n",
       " '1776205': 0.4341061334562677,\n",
       " '639669': 0.38760818984443224,\n",
       " '8448903': 0.4124776432310974,\n",
       " '8448902': 0.41143788988374125,\n",
       " '639670': 0.3823782759630887,\n",
       " '639663': 0.3775039059463895,\n",
       " '35722': 0.39951390916561724,\n",
       " '1302853': 0.36679545440286726,\n",
       " '639671': 0.36361852471115813,\n",
       " '1786523': 0.36268165971062455,\n",
       " '588394': 0.38289955583934543,\n",
       " '639690': 0.3653150559189846,\n",
       " '1450640': 0.38388484121055044,\n",
       " '3936360': 0.3864522671342978,\n",
       " '5622935': 0.3518175514979561,\n",
       " '2992576': 0.353630291595394,\n",
       " '47133': 0.3731257557936975,\n",
       " '47129': 0.34765838000119853}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_ranker_bm25_model1.score_candidates(query_res[1], query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  A an example of a ranker that uses averaged embeddings (loading embeddings can take a couple of minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_ranker_avg_embed = JavaQueryRanker(resource_manager, \n",
    "                                          feat_extr_file_name='exper_desc.best/extractors/avgembed.json', \n",
    "                                          model_file_name='exper_desc.best/models/one_feat.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'639661': 0.0,\n",
       " '472789': 0.0,\n",
       " '1776205': 0.0,\n",
       " '639669': 0.0,\n",
       " '8448903': 0.0,\n",
       " '8448902': 0.0,\n",
       " '639670': 0.0,\n",
       " '639663': 0.0,\n",
       " '35722': 0.0,\n",
       " '1302853': 0.0,\n",
       " '639671': 0.0,\n",
       " '1786523': 0.0,\n",
       " '588394': 0.0,\n",
       " '639690': 0.0,\n",
       " '1450640': 0.0,\n",
       " '3936360': 0.0,\n",
       " '5622935': 0.0,\n",
       " '2992576': 0.0,\n",
       " '47133': 0.0,\n",
       " '47129': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_ranker_avg_embed.score_candidates(query_res[1], query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A toy example where we re-rank the list of candidate using a BERT re-ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-ranking on CPU, which can be fairly slow\n",
    "neural_ranker = PythonNNQueryRanker(resource_manager, \n",
    "                         query_field_name='text_raw', max_query_len=64,\n",
    "                         index_field_name='text_raw', max_doc_len = 512 - 32 - 3,\n",
    "                         device_name='cuda', batch_size=25, \n",
    "                         model_file_name=f'derived_data/ir_models/vanilla_bert/model.best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('639661', 3.1647984981536865),\n",
       " ('639690', 2.259718656539917),\n",
       " ('472789', 2.1923577785491943),\n",
       " ('639670', 2.032242774963379),\n",
       " ('35722', 1.9418951272964478),\n",
       " ('1786523', 1.8832405805587769),\n",
       " ('639671', 1.755630373954773),\n",
       " ('639669', 1.7364437580108643),\n",
       " ('639663', 1.7072409391403198),\n",
       " ('1776205', 1.663066029548645),\n",
       " ('8448903', 1.4759409427642822),\n",
       " ('3936360', 1.3319076299667358),\n",
       " ('588394', 1.2384809255599976),\n",
       " ('1450640', 1.1465526819229126),\n",
       " ('8448902', 1.131631851196289),\n",
       " ('1302853', 0.8631684184074402),\n",
       " ('47129', 0.8423947095870972),\n",
       " ('5622935', 0.798784613609314),\n",
       " ('47133', 0.6458267569541931),\n",
       " ('2992576', 0.380513995885849)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_dict = {DOCID_FIELD : FAKE_QUERY_ID, \n",
    "              'text_raw' : QUERY_TEXT}\n",
    "neural_ranker.rank_candidates(query_res[1], query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'639661': 3.1647984981536865,\n",
       " '472789': 2.1923577785491943,\n",
       " '1776205': 1.663066029548645,\n",
       " '639669': 1.7364437580108643,\n",
       " '8448903': 1.4759409427642822,\n",
       " '8448902': 1.131631851196289,\n",
       " '639670': 2.032242774963379,\n",
       " '639663': 1.7072409391403198,\n",
       " '35722': 1.9418951272964478,\n",
       " '1302853': 0.8631684184074402,\n",
       " '639671': 1.755630373954773,\n",
       " '1786523': 1.8832405805587769,\n",
       " '588394': 1.2384809255599976,\n",
       " '639690': 2.259718656539917,\n",
       " '1450640': 1.1465526819229126,\n",
       " '3936360': 1.3319076299667358,\n",
       " '5622935': 0.798784613609314,\n",
       " '2992576': 0.380513995885849,\n",
       " '47133': 0.6458267569541931,\n",
       " '47129': 0.8423947095870972}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_dict = {DOCID_FIELD : FAKE_QUERY_ID, \n",
    "              'text_raw' : QUERY_TEXT}\n",
    "neural_ranker.score_candidates(query_res[1], query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CandidateEntry(doc_id='639661', score=18.328275680541992),\n",
       " CandidateEntry(doc_id='472789', score=16.816619873046875),\n",
       " CandidateEntry(doc_id='1776205', score=16.630727767944336),\n",
       " CandidateEntry(doc_id='639669', score=15.6367826461792),\n",
       " CandidateEntry(doc_id='8448903', score=15.448601722717285),\n",
       " CandidateEntry(doc_id='8448902', score=15.369601249694824),\n",
       " CandidateEntry(doc_id='639670', score=15.27547550201416),\n",
       " CandidateEntry(doc_id='639663', score=14.904623985290527),\n",
       " CandidateEntry(doc_id='35722', score=14.59425163269043),\n",
       " CandidateEntry(doc_id='1302853', score=14.318553924560547),\n",
       " CandidateEntry(doc_id='639671', score=14.157160758972168),\n",
       " CandidateEntry(doc_id='1786523', score=14.077558517456055),\n",
       " CandidateEntry(doc_id='588394', score=13.997241973876953),\n",
       " CandidateEntry(doc_id='639690', score=13.810718536376953),\n",
       " CandidateEntry(doc_id='1450640', score=13.643953323364258),\n",
       " CandidateEntry(doc_id='3936360', score=13.642525672912598),\n",
       " CandidateEntry(doc_id='5622935', score=13.536111831665039),\n",
       " CandidateEntry(doc_id='2992576', score=13.500545501708984),\n",
       " CandidateEntry(doc_id='47133', score=13.166474342346191),\n",
       " CandidateEntry(doc_id='47129', score=13.15163803100586)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A comprehensive example where we evaluate **all** queries from `dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_convert.convert_common import *\n",
    "all_queries = read_queries(QUERY_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'DOCNO': 'dev_0',\n",
       "  'text': 'vein carry blood heart away',\n",
       "  'text_unlemm': 'veins carry blood heart away',\n",
       "  'text_raw': 'do veins carry blood to the heart or away',\n",
       "  'answer_list': ['to'],\n",
       "  'text_bert_tok': 'do veins carry blood to the heart or away'},\n",
       " {'DOCNO': 'dev_1',\n",
       "  'text': 'sister king country',\n",
       "  'text_unlemm': 'sister king country',\n",
       "  'text_raw': 'who is the sister of for king and country',\n",
       "  'answer_list': ['Rebecca St. James'],\n",
       "  'text_bert_tok': 'who is the sister of for king and country'},\n",
       " {'DOCNO': 'dev_2',\n",
       "  'text': 'develop periodic table 8 column',\n",
       "  'text_unlemm': 'developed periodic table 8 columns',\n",
       "  'text_raw': 'who developed the first periodic table with 8 columns',\n",
       "  'answer_list': ['Dmitri Mendeleev'],\n",
       "  'text_bert_tok': 'who developed the first periodic table with 8 columns'},\n",
       " {'DOCNO': 'dev_3',\n",
       "  'text': 'season 14 grey anatomy come',\n",
       "  'text_unlemm': 'season 14 grey anatomy come',\n",
       "  'text_raw': \"when does season 14 of grey 's anatomy come out\",\n",
       "  'answer_list': ['September 28 , 2017'],\n",
       "  'text_bert_tok': \"when does season 14 of grey ' s anatomy come out\"},\n",
       " {'DOCNO': 'dev_4',\n",
       "  'text': 'big statue jesus locate',\n",
       "  'text_unlemm': 'big statue jesus located',\n",
       "  'text_raw': 'where is the big statue of jesus located',\n",
       "  'answer_list': ['Rio de Janeiro , Brazil'],\n",
       "  'text_bert_tok': 'where is the big statue of jesus located'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query sample\n",
    "all_queries[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries have one extra field that cannot be \"digested\" by the ranking API and we need to delete it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:00<00:00, 702704.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for query_dict in tqdm(all_queries):\n",
    "    # Delete this field, it cannot be used by ranker\n",
    "    del query_dict['answer_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 96.98it/s]      \n",
      "  1%|▏         | 7/500 [00:00<00:07, 65.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0.23153725283035664\n",
      "0.2611430435620929\n",
      "0.1882523067559224\n",
      "0.3165160815593989\n",
      "===========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:06<00:00, 80.84it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0.4640738457712773\n",
      "0.49249668209855113\n",
      "0.3838658604949658\n",
      "0.5713954663360119\n",
      "===========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [05:01<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0.5581757423424439\n",
      "0.5704311241226674\n",
      "0.46750163760495106\n",
      "0.6797521001099326\n",
      "===========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts.eval_common import *\n",
    "from scripts.utils import sync_out_streams\n",
    "from tqdm import tqdm\n",
    "\n",
    "TOP_K=50\n",
    "MAX_QUERIES_QTY=500\n",
    "qrels=read_qrels_dict(QREL_FILE_NAME)\n",
    "\n",
    "for ranker in [java_ranker_avg_embed, java_ranker_bm25_model1, neural_ranker]:\n",
    "    run_dict = {}\n",
    "    with tqdm(all_queries[0:MAX_QUERIES_QTY]) as pbar:\n",
    "        for query_dict in pbar:\n",
    "            qid = query_dict[DOCID_FIELD]\n",
    "            query_res = run_query(cand_prov, TOP_K, query_dict)\n",
    "            rank_res = ranker.score_candidates(query_res[1], query_dict)\n",
    "            run_dict[qid] = rank_res\n",
    "    tqdm.write('\\n')\n",
    "        \n",
    "    # Let us compute various metrics using our Python code. \n",
    "    # Note that results should generally match results obtained using `scripts/exper/run_experiments.sh`\n",
    "    for eval_obj in [NormalizedDiscountedCumulativeGain(10), \\\n",
    "                 NormalizedDiscountedCumulativeGain(20), \\\n",
    "                 MeanAveragePrecision(), \\\n",
    "                 MeanReciprocalRank()]:\n",
    "        tqdm.write(str(eval_run(rerank_run=run_dict, metric_func=eval_obj, qrels_dict=qrels)))\n",
    "    \n",
    "    tqdm.write('==========================='+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optionally we can save the run to be later evaluated using external evaluation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_run_dict(run_dict, 'run.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head run.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
