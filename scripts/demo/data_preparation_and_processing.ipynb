{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation/downloading/processing notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to move to the top-level directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works with a sub-sample of the natural question collection (__Wikipedia DPR__) prepared by [Karpukhin et al.](https://github.com/facebookresearch/DPR). This subset includes all the questions, but only about one million Wikipedia passages. The generation of this subset is briefly described below, but for your convenience we provide an archive with already processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-01 14:37:38--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_2021-06-30.tar.bz2\n",
      "Resolving boytsov.info (boytsov.info)... 69.60.127.165\n",
      "Connecting to boytsov.info (boytsov.info)|69.60.127.165|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2766256898 (2.6G) [application/x-bzip2]\n",
      "Saving to: ‘wikipedia_dpr_nq_sample_2021-06-30.tar.bz2’\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>]   2.58G  16.8MB/s    in 3m 15s  \n",
      "\n",
      "2021-07-01 14:40:53 (13.5 MB/s) - ‘wikipedia_dpr_nq_sample_2021-06-30.tar.bz2’ saved [2766256898/2766256898]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_2021-06-30.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x wikipedia_dpr_nq_sample/input_data/\n",
      "x wikipedia_dpr_nq_sample/input_data/train_fusion/\n",
      "x wikipedia_dpr_nq_sample/input_data/train_fusion/QuestionFields.jsonl\n",
      "x wikipedia_dpr_nq_sample/input_data/train_fusion/qrels.txt\n",
      "x wikipedia_dpr_nq_sample/input_data/train_fusion/QuestionFields.bin\n",
      "x wikipedia_dpr_nq_sample/input_data/dev/\n",
      "x wikipedia_dpr_nq_sample/input_data/dev/QuestionFields.jsonl\n",
      "x wikipedia_dpr_nq_sample/input_data/dev/qrels.txt\n",
      "x wikipedia_dpr_nq_sample/input_data/dev/QuestionFields.bin\n",
      "x wikipedia_dpr_nq_sample/input_data/bitext/\n",
      "x wikipedia_dpr_nq_sample/input_data/bitext/QuestionFields.jsonl\n",
      "x wikipedia_dpr_nq_sample/input_data/bitext/qrels.txt\n",
      "x wikipedia_dpr_nq_sample/input_data/pass_sample/\n",
      "x wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.bin\n",
      "x wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.jsonl.gz\n",
      "x wikipedia_dpr_nq_sample/input_data/dev_official/\n",
      "x wikipedia_dpr_nq_sample/input_data/dev_official/QuestionFields.jsonl\n",
      "x wikipedia_dpr_nq_sample/input_data/dev_official/qrels.txt\n",
      "x wikipedia_dpr_nq_sample/input_data/dev_official/QuestionFields.bin\n",
      "x wikipedia_dpr_nq_sample/exper_desc/\n",
      "x wikipedia_dpr_nq_sample/exper_desc/extractors/\n",
      "x wikipedia_dpr_nq_sample/exper_desc/extractors/bm25_ance_model1.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc/extractors/bm25_ance.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc/extractors/bm25_cedr8080.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc/extractors/ance.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc/bm25_ance_model1.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc/bm25_ance.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc/lucene.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc/bm25_cedr8080.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc/bm25.json\n",
      "x wikipedia_dpr_nq_sample/exper_desc/models/\n",
      "x wikipedia_dpr_nq_sample/exper_desc/models/one_feat.model\n",
      "x wikipedia_dpr_nq_sample/exper_desc/ance.json\n",
      "x wikipedia_dpr_nq_sample/model_conf/vanilla_bert.json\n",
      "x wikipedia_dpr_nq_sample/derived_data/bitext/answer_text_bert_tok\n",
      "x wikipedia_dpr_nq_sample/derived_data/bitext/question_text_bert_tok\n",
      "x wikipedia_dpr_nq_sample/derived_data/bitext/answer_text_unlemm\n",
      "x wikipedia_dpr_nq_sample/derived_data/bitext/question_text_unlemm\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_2021-06-30.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv wikipedia_dpr_nq_sample collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carry out a basic sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: collections\n",
      "Checking input sub-directory: bitext\n",
      "Checking input sub-directory: dev\n",
      "Checking input sub-directory: dev_official\n",
      "Checking input sub-directory: pass_sample\n",
      "Found indexable data file: pass_sample/AnswerFields.jsonl.gz\n",
      "Checking input sub-directory: train_fusion\n",
      "Found query file: bitext/QuestionFields.jsonl\n",
      "Found query file: dev/QuestionFields.jsonl\n",
      "Found query file: dev_official/QuestionFields.jsonl\n",
      "Found query file: train_fusion/QuestionFields.jsonl\n",
      "getIndexQueryDataDirs return value:  pass_sample AnswerFields.jsonl.gz bitext,dev,dev_official,train_fusion\n",
      "Using data file: AnswerFields.jsonl.gz\n",
      "Index dirs: pass_sample\n",
      "Query dirs: bitext dev dev_official train_fusion\n",
      "Queries/questions:\n",
      "bitext 53880\n",
      "dev 2500\n",
      "dev_official 6515\n",
      "train_fusion 2500\n",
      "Documents/passages/answers:\n",
      "pass_sample 774392\n"
     ]
    }
   ],
   "source": [
    "!scripts/report/get_basic_collect_stat.sh wikipedia_dpr_nq_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing in more details\n",
    "### Most users can ignore this part (at least the first time the study these notebooks) and proceed to notebooks that describe indexing and model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The download and conversion script can be found in the directory `scripts/data_convert/wikipedia_dpr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p collections/wikipedia_dpr_nq_sample/input_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download passages and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/data_convert/wikipedia_dpr/download_dpr_passages.sh collections/wikipedia_dpr_nq_sample/input_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/data_convert/wikipedia_dpr/download_dpr_queries.sh nq collections/wikipedia_dpr_nq_sample/input_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly split the training set into the new training and development sets. This script also converts the data into FlexNeuART format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/data_convert/wikipedia_dpr/split_and_convert_dpr_queries.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    collections/wikipedia_dpr_nq_sample/input_raw/ \\\n",
    "    nq \\\n",
    "    -partition_sizes ,5000,2500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The split & convert script produces outputs of two types:\n",
    "1. The set of questions in JSONL format. These questions are divided into several subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mbitext\u001b[m\u001b[m       \u001b[34mdev\u001b[m\u001b[m          \u001b[34mdev_official\u001b[m\u001b[m \u001b[34mpass_sample\u001b[m\u001b[m  \u001b[34mtrain_fusion\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls collections/wikipedia_dpr_nq_sample/input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bitext` subset and the `train_fusion` subsets are supposed to be used to train models. The difference is that `train_fusion` is a smaller subset that can be used to create fusion models. The `bitext` part can be used to train, e.g., neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the queries from the `bitext` set, the conversion script creates parallel data (bitext) where questions are aligned with respective answer-bearing sentences. We create three parallel corpora that correspond to three ways to lemmatize & tokenize input (lemmas and original tokens with stopwords removed and BERT-tokenized text). They are stored in the `derived_data/bitext` subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_text_bert_tok   question_text_bert_tok\r\n",
      "answer_text_unlemm     question_text_unlemm\r\n"
     ]
    }
   ],
   "source": [
    "!ls collections/wikipedia_dpr_nq_sample/derived_data/bitext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding document and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We already __ship__ data with documents and queries (except for the bitext part) embedded using an [ANCE Wikipedia model](https://github.com/microsoft/ANCE). This is done using the scripts in the `scripts/data_convert/ance` directory.\n",
    "2. First, one needs to download the models using the script `scripts/data_convert/ance/download_ance_models.sh`.\n",
    "3. Then, one can embed documents using a command like this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "scripts/data_convert/ance/embed.py \\\n",
    "    --input collections/wikipedia_dpr_nq_sample/input_raw/psgs_w100.tsv.gz \\\n",
    "    --output collections/wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.bin \\\n",
    "    --field_name dense  \\\n",
    "    --model_dir <model download directory> \\\n",
    "    --data_type dpr_nq \\\n",
    "    --doc_ids collections/wikipedia_dpr_nq_sample/input_raw/nq_selected_psg_ids.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ... and queries using a command like this one (note we specify __the binary field name__):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for part in train_fusion dev dev_official ; do \\\n",
    "    scripts/data_convert/ance/embed.py \\\n",
    "        --input collections/wikipedia_dpr_nq_sample/input_data/$part/QuestionFields.jsonl \\\n",
    "        --output collections/wikipedia_dpr_nq_sample/input_data/$part/QuestionFields.bin \\\n",
    "        --field_name dense  \\\n",
    "        --model_dir <model download directory> \\\n",
    "        --data_type dpr_nq \n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
