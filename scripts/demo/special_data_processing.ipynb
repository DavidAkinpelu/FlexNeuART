{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to move to the top-level directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leo/SourceTreeGit/FlexNeuART.refact2021\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA data: weak supervision with answer-based QRELs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of QA the set of relevance passages is obtained by retrieving a top-K set of passages using a candidate provider and checking if the passages contain an answer as a substring. Facebook Wikipedia DPR data is shipped with relevance information obtain in such as a way. However, not all collections are. Furthermore, this data depends a lot on the quality of a candidate generator. Ideally, when multiple retrieval systems are used and comapred, the sets of  relevance documents (from their respective top-k sets) need to be combined (i.e., **pooled**). Our framework does support such a functionality. To this end, each query entry in a JSONL file needs to have special field \"answer_list\", e.g.:\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"DOCNO\": \"dev_official_0\",\n",
    "    \"text\": \"sing love reba\",\n",
    "    \"text_raw\": \"who sings does he love me with reba\",\n",
    "    \"answer_list\": [\n",
    "        \"Linda Davis\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Then the respective set of QRELs can be generated using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/data_convert/create_answ_based_qrels.sh  \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    bitext \\\n",
    "    text_raw \\\n",
    "    qrels_generated_from_bitext_queries.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating parallel corpus (bitext) without explicitly paired data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of queries paired with a set of short relevant passages can be used to train a lexical IBM Model 1 model whose fusion with BM25 can be quite effective as a ranking model. In the case of QA data, such as corpus can be easily created by pairing questions with sentences containing an answer. This is what we do when we process the Wikipedia DPR corpus. However, such pairing generally does not exist for more generic ad hoc retrieval collections. It can still be possible to create a reasonable quality paired data by splitting a relevant passage into multiple short chunks and pairing each chunk with the respective queries. This works especially well for short passages or short information snippets such as titles, urls, or headings.\n",
    "\n",
    "Here is an example of creating such an artificial bitext corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/giza/export_bitext_plain.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    text_bert_tok text_bert_tok \\\n",
    "    2 \\\n",
    "    -bitext_out_subdir bitext_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train the Model 1 model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/giza/create_tran.sh wikipedia_dpr_nq_sample text_bert_tok \\\n",
    "   -bitext_subdir bitext_generated \\\n",
    "   -model1_subdir giza_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prune the translation table and store it in a special format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!min_tran_prob=0.001 ; top_word_qty=1000000 ; echo $min_tran_prob ; top_word_qty=100000 ; \\\n",
    "scripts/giza/filter_tran_table_and_voc.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    text_bert_tok \\\n",
    "    $min_tran_prob \\\n",
    "    $top_word_qty \\\n",
    "    -model1_subdir giza_generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
