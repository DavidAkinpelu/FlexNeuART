{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation: Training & testing fusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two things to do before we start:\n",
    "1. Point environment variable `COLLECT_ROOT` to the collection root.\n",
    "2. Change directory to the location of installed scripts/binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COLLECT_ROOT=/home/leo/flexneuart_collections\n"
     ]
    }
   ],
   "source": [
    "%env COLLECT_ROOT=/home/leo/flexneuart_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leo/flexneuart_scripts\n"
     ]
    }
   ],
   "source": [
    "cd /home/leo/flexneuart_scripts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a fusion of BM25 and Model1 using the optimal configuration obtained during the fine-tuning step\n",
    "\n",
    "Training uses only the first 5000 queries from the fusion set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: /home/leo/flexneuart_collections\n",
      "The number of CPU cores:      8\n",
      "The number of || experiments: 1\n",
      "The number of threads:        8\n",
      "================================================================================\n",
      "Experiment descriptor file:                                 /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/exper_desc.best/bm25_model1.json\n",
      "Default test set:                                           dev\n",
      "Number of parallel experiments:                             1\n",
      "Number of threads in feature extractors/query applications: 8\n",
      "================================================================================\n",
      "Parsed experiment parameters:\n",
      "experSubdir:feat_exper/bm25_model1\n",
      "extrTypeFinal:exper_desc.best/extractors/bm25=text+model1=text_bert_tok+lambda=0.3+probSelfTran=0.35.json\n",
      "testOnly:0\n",
      "========================================\n",
      "Started a process 9538, working dir: /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_model1\n",
      "Process log file: /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_model1/exper.log\n",
      "Waiting for 1 child processes\n",
      "Process with pid=9538 finished successfully.\n",
      "Waiting for 0 child processes\n",
      "================================================================================\n",
      "1 experiments executed\n",
      "0 experiments failed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!./exper/run_experiments.sh \\\n",
    "   wikipedia_dpr_nq_sample \\\n",
    "   exper_desc.best/bm25_model1.json \\\n",
    "   -max_num_query_train 5000 \\\n",
    "   -train_cand_qty 20 \\\n",
    "   -test_part dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-100 report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of queries:    2500\r\n",
      "NDCG@10:  0.499600\r\n",
      "NDCG@20:  0.535100\r\n",
      "NDCG@100: 0.598100\r\n",
      "P20:      0.199400\r\n",
      "MAP:      0.439000\r\n",
      "MRR:      0.584700\r\n",
      "Recall:   0.889732\r\n"
     ]
    }
   ],
   "source": [
    "!cat $COLLECT_ROOT/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_model1/rep/out_100.rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a fusion of (query-normalized) BM25 and BERT-model scores\n",
    "One needs to start a query server that binds to the port 8080 as shown below. This needs to be done in __a separate terminal__, because notebooks do not support background processes. Please, note we have to specify __the same maximum query and document lengths__ as during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "COLLECT_ROOT=/home/leo/flexneuart_collections\n",
    "\n",
    "./featextr_server/nn_rank_server.py  \\\n",
    "   --init_model $COLLECT_ROOT/wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/model.best \\\n",
    "   --port 8080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run an experiment by training using a `train_fusion` subset of queries and testing on the `dev` subset. Please, note the following:\n",
    "1. During training time use 20 candidates, but for testing on `dev` we re-rank 50 candidates. The ranking of candidates below 50th position will not change.\n",
    "2. We use two threads and output log to the screen (i.e., the process is no started in a separate shell).\n",
    "3. Training uses only the __first 5000__ queries from the fusion set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: /home/leo/flexneuart_collections\n",
      "The number of CPU cores:      8\n",
      "The number of || experiments: 1\n",
      "The number of threads:        2\n",
      "================================================================================\n",
      "Experiment descriptor file:                                 /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/exper_desc.best/bm25_cedr8080.json\n",
      "Default test set:                                           dev\n",
      "Number of parallel experiments:                             1\n",
      "Number of threads in feature extractors/query applications: 2\n",
      "================================================================================\n",
      "Parsed experiment parameters:\n",
      "experSubdir:feat_exper/bm25_cedr8080\n",
      "extrTypeFinal:exper_desc.best/extractors/bm25_cedr8080.json\n",
      "testOnly:0\n",
      "========================================\n",
      "Experimental directory already exists (ignoring): /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_cedr8080\n",
      "Waiting for 0 child processes\n",
      "================================================================================\n",
      "0 experiments executed\n",
      "0 experiments failed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!./exper/run_experiments.sh \\\n",
    "   wikipedia_dpr_nq_sample \\\n",
    "   exper_desc.best/bm25_cedr8080.json \\\n",
    "   -max_num_query_train 5000 \\\n",
    "   -train_cand_qty 20 \\\n",
    "   -max_final_rerank_qty 50 \\\n",
    "   -test_part dev \\\n",
    "   -thread_qty 2 \\\n",
    "   -no_separate_shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results are available in the directory `collections/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_cedr8080`.\n",
    "\n",
    "The following is a summary report (top-100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of queries:    2500\r\n",
      "NDCG@10:  0.579000\r\n",
      "NDCG@20:  0.590100\r\n",
      "NDCG@100: 0.630900\r\n",
      "P20:      0.192400\r\n",
      "MAP:      0.504800\r\n",
      "MRR:      0.687700\r\n",
      "Recall:   0.808185\r\n"
     ]
    }
   ],
   "source": [
    "!cat $COLLECT_ROOT/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_cedr8080/rep/out_100.rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a fusion of BM25 and dense-embeddings (ANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: /home/leo/flexneuart_collections\n",
      "The number of CPU cores:      8\n",
      "The number of || experiments: 1\n",
      "The number of threads:        8\n",
      "================================================================================\n",
      "Experiment descriptor file:                                 /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/exper_desc.best/bm25_ance.json\n",
      "Default test set:                                           dev\n",
      "Number of parallel experiments:                             1\n",
      "Number of threads in feature extractors/query applications: 8\n",
      "================================================================================\n",
      "Parsed experiment parameters:\n",
      "experSubdir:feat_exper/bm25_ance\n",
      "candProvAddConf:exper_desc.best/lucene.json\n",
      "extrTypeFinal:exper_desc.best/extractors/bm25_ance.json\n",
      "testOnly:0\n",
      "========================================\n",
      "Started a process 10057, working dir: /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_ance\n",
      "Process log file: /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_ance/exper.log\n",
      "Waiting for 1 child processes\n",
      "Process with pid=10057 finished successfully.\n",
      "Waiting for 0 child processes\n",
      "================================================================================\n",
      "1 experiments executed\n",
      "0 experiments failed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!./exper/run_experiments.sh \\\n",
    "   wikipedia_dpr_nq_sample \\\n",
    "   exper_desc.best/bm25_ance.json \\\n",
    "   -max_num_query_train 5000 \\\n",
    "   -train_cand_qty 20 \\\n",
    "   -test_part dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-100 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of queries:    2500\r\n",
      "NDCG@10:  0.655400\r\n",
      "NDCG@20:  0.657200\r\n",
      "NDCG@100: 0.698200\r\n",
      "P20:      0.155500\r\n",
      "MAP:      0.561800\r\n",
      "MRR:      0.865900\r\n",
      "Recall:   0.652528\r\n"
     ]
    }
   ],
   "source": [
    "!cat $COLLECT_ROOT/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_ance/rep/out_100.rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a fusion of BM25 and dense-embeddings (averaged glove embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: /home/leo/flexneuart_collections\n",
      "The number of CPU cores:      8\n",
      "The number of || experiments: 1\n",
      "The number of threads:        8\n",
      "================================================================================\n",
      "Experiment descriptor file:                                 /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/exper_desc.best/bm25_avgembed.json\n",
      "Default test set:                                           dev\n",
      "Number of parallel experiments:                             1\n",
      "Number of threads in feature extractors/query applications: 8\n",
      "================================================================================\n",
      "Parsed experiment parameters:\n",
      "experSubdir:feat_exper/bm25_avgembed\n",
      "candProvAddConf:exper_desc.best/lucene.json\n",
      "extrTypeFinal:exper_desc.best/extractors/bm25_avgembed.json\n",
      "testOnly:0\n",
      "========================================\n",
      "Started a process 10252, working dir: /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_avgembed\n",
      "Process log file: /home/leo/flexneuart_collections/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_avgembed/exper.log\n",
      "Waiting for 1 child processes\n",
      "Process with pid=10252 finished successfully.\n",
      "Waiting for 0 child processes\n",
      "================================================================================\n",
      "1 experiments executed\n",
      "0 experiments failed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!./exper/run_experiments.sh \\\n",
    "   wikipedia_dpr_nq_sample \\\n",
    "   exper_desc.best/bm25_avgembed.json \\\n",
    "   -max_num_query_train 5000 \\\n",
    "   -train_cand_qty 20 \\\n",
    "   -test_part dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-100 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of queries:    2500\r\n",
      "NDCG@10:  0.402400\r\n",
      "NDCG@20:  0.434900\r\n",
      "NDCG@100: 0.507700\r\n",
      "P20:      0.165600\r\n",
      "MAP:      0.345700\r\n",
      "MRR:      0.486800\r\n",
      "Recall:   0.819776\r\n"
     ]
    }
   ],
   "source": [
    "!cat $COLLECT_ROOT/wikipedia_dpr_nq_sample/results/dev/feat_exper/bm25_avgembed/rep/out_100.rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on efficient feature generation\n",
    "1. Generating certain features, in particular, scores from large models is expensive. However, these features are not always useful. For example, you can train two similar models. Combining them in the ensemble can lead to no improvement and even cause some minor degradation. \n",
    "2. Furthermore, the outcomes of applying the coordinate ascent model is affected by randomness. Running  the full training and testing pipeine using different sets of features can be extremely expensive.\n",
    "3. This can be avoided with some effort. To this end, one needs to set the flag `trainOnly` in the experimental descriptor (see above documentation) to `true` and run the experimental pipeline twice using **different** values of parameters `-train_cand_qty` and `-train_part`. In the first case, you specify your actual training set. In the second case, the option `-train_part` should point to your test/validation set. Usually, I use a smaller number of candidates (`-train_cand_qty`) when I generate features for the training part.\n",
    "4. Note that in the training-only model, there is no need to specify the test part.\n",
    "4. As a result, you will have two sets of features that can be used for training and validation. Use logs to locate these features (eventually then end up being in the sub-directory `letor`).\n",
    "5. Now one can use the RankLib library directly. Check out the options of this library by executing:\n",
    "```\n",
    "java -jar lib/RankLib.jar \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
