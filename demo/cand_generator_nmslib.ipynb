{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using k-NN search on dense and dense-sparse representation for candidate generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to move to the top-level directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leo/SourceTreeGit/FlexNeuART.refact2021\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating purely dense (ANCE) embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data (currently we have no convenience wrapper scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p collections/wikipedia_dpr_nq_sample/derived_data/nmslib/ance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.resources.ResourceManager - Resource manager initialization. Resource root:\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.MapDbBackend - MapDB opened for reading: collections/wikipedia_dpr_nq_sample/forward_index/dense.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.ForwardIndexBinaryDataDict - Finished loading context from file: collections/wikipedia_dpr_nq_sample/forward_index/dense.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.FeatExtrDenseDocEmbedDotProdSimilarity - Index field name: dense normalize embeddings:? false\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Writing the number of entries (774392) to the output file\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 100000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 200000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 300000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 400000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 500000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 600000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 700000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 774392 docs\n"
     ]
    }
   ],
   "source": [
    "!target/appassembler/bin/ExportToNMSLIBDenseSparseFusion  \\\n",
    "    -extr_json collections/wikipedia_dpr_nq_sample/exper_desc.best/extractors/ance.json \\\n",
    "    -fwd_index_dir collections/wikipedia_dpr_nq_sample/forward_index/ \\\n",
    "    -out_file collections/wikipedia_dpr_nq_sample/derived_data/nmslib/ance/export.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compile the latest NMSLIB [query server (details)](https://github.com/nmslib/nmslib/blob/master/manual/query_server.md). Then it needs to be started from the FlexNeuART source tree root as shown below. In our example, the server is going to carry out the brute-force search. It is also possible to create an index using, e.g., HNSW:\n",
    "\n",
    "```\n",
    "<path to the query server>/query_server \\\n",
    "    -p 8000 \\\n",
    "     -s sparse_dense_fusion:weightfilename=\\\n",
    "collections/wikipedia_dpr_nq_sample/exper_desc.best/nmslib/ance/fusion_weights \\\n",
    "     -i collections/wikipedia_dpr_nq_sample/derived_data/nmslib/ance/export.data \\\n",
    "     -m brute_force\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Now we can run a benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: collections\n",
      "The number of CPU cores:      8\n",
      "The number of || experiments: 1\n",
      "The number of threads:        8\n",
      "================================================================================\n",
      "Experiment descriptor file:                                 collections/wikipedia_dpr_nq_sample/exper_desc.best/nmslib_ance.json\n",
      "Default test set:                                           dev\n",
      "Number of parallel experiments:                             1\n",
      "Number of threads in feature extractors/query applications: 8\n",
      "================================================================================\n",
      "Parsed experiment parameters:\n",
      "experSubdir:final_exper/ance_knn\n",
      "candProv:nmslib\n",
      "candProvAddConf:exper_desc.best/nmslib/ance/cand_prov.json\n",
      "candProvURI:localhost:8000\n",
      "candQty:1000\n",
      "testOnly:1\n",
      "========================================\n",
      "Started a process 2177, working dir: collections/wikipedia_dpr_nq_sample/results/dev/final_exper/ance_knn\n",
      "Process log file: collections/wikipedia_dpr_nq_sample/results/dev/final_exper/ance_knn/exper.log\n",
      "Waiting for 1 child processes\n",
      "Process with pid=2177 finished successfully.\n",
      "Waiting for 0 child processes\n",
      "================================================================================\n",
      "1 experiments executed\n",
      "0 experiments failed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!scripts/exper/run_experiments.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    exper_desc.best/nmslib_ance.json \\\n",
    "    -test_part dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top-100 report:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of queries:    2500\r\n",
      "NDCG@10:  0.705800\r\n",
      "NDCG@20:  0.702400\r\n",
      "NDCG@100: 0.728700\r\n",
      "P20:      0.145400\r\n",
      "MAP:      0.595500\r\n",
      "MRR:      0.946100\r\n",
      "Recall:   0.546620\r\n"
     ]
    }
   ],
   "source": [
    "!cat collections/wikipedia_dpr_nq_sample/results/dev/final_exper/ance_knn/rep/out_100.rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating mixed dense-sparse (BM25+ANCE) embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data (currently we have no convenience wrapper scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p collections/wikipedia_dpr_nq_sample/derived_data/nmslib/bm25_ance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.resources.ResourceManager - Resource manager initialization. Resource root:\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.MapDbBackend - MapDB opened for reading: collections/wikipedia_dpr_nq_sample/forward_index/text.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.ForwardIndexBinaryDataDict - Finished loading context from file: collections/wikipedia_dpr_nq_sample/forward_index/text.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.MapDbBackend - MapDB opened for reading: collections/wikipedia_dpr_nq_sample/forward_index/dense.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.ForwardIndexBinaryDataDict - Finished loading context from file: collections/wikipedia_dpr_nq_sample/forward_index/dense.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.FeatExtrDenseDocEmbedDotProdSimilarity - Index field name: dense normalize embeddings:? false\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Writing the number of entries (774392) to the output file\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 100000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 200000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 300000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 400000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 500000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 600000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 700000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 774392 docs\n"
     ]
    }
   ],
   "source": [
    "!target/appassembler/bin/ExportToNMSLIBDenseSparseFusion  \\\n",
    "    -extr_json collections/wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_ance.json \\\n",
    "    -fwd_index_dir collections/wikipedia_dpr_nq_sample/forward_index/ \\\n",
    "    -out_file collections/wikipedia_dpr_nq_sample/derived_data/nmslib/bm25_ance/export.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compile the latest NMSLIB [query server (details)](https://github.com/nmslib/nmslib/blob/master/manual/query_server.md). Then it needs to be started from the FlexNeuART source tree root as shown below. In our example, the server is going to carry out the brute-force search. It is also possible to create an index using, e.g., HNSW:\n",
    "\n",
    "```\n",
    "<path to the query server>/query_server \\\n",
    "    -p 8000 \\\n",
    "     -s sparse_dense_fusion:weightfilename=\\\n",
    "collections/wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance/fusion_weights \\\n",
    "     -i collections/wikipedia_dpr_nq_sample/derived_data/nmslib/bm25_ance/export.data \\\n",
    "     -m brute_force\n",
    "```\n",
    "\n",
    "Now we can run a benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: collections\n",
      "The number of CPU cores:      8\n",
      "The number of || experiments: 1\n",
      "The number of threads:        8\n",
      "================================================================================\n",
      "Experiment descriptor file:                                 collections/wikipedia_dpr_nq_sample/exper_desc.best/nmslib_bm25_ance.json\n",
      "Default test set:                                           dev\n",
      "Number of parallel experiments:                             1\n",
      "Number of threads in feature extractors/query applications: 8\n",
      "================================================================================\n",
      "Parsed experiment parameters:\n",
      "experSubdir:final_exper/bm25_ance_knn\n",
      "candProv:nmslib\n",
      "candProvAddConf:exper_desc.best/nmslib/bm25_ance/cand_prov.json\n",
      "candProvURI:localhost:8000\n",
      "candQty:1000\n",
      "testOnly:1\n",
      "========================================\n",
      "Started a process 2345, working dir: collections/wikipedia_dpr_nq_sample/results/dev/final_exper/bm25_ance_knn\n",
      "Process log file: collections/wikipedia_dpr_nq_sample/results/dev/final_exper/bm25_ance_knn/exper.log\n",
      "Waiting for 1 child processes\n",
      "Process with pid=2345 finished successfully.\n",
      "Waiting for 0 child processes\n",
      "================================================================================\n",
      "1 experiments executed\n",
      "0 experiments failed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!scripts/exper/run_experiments.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    exper_desc.best/nmslib_bm25_ance.json \\\n",
    "    -test_part dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top 100 report__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of queries:    2500\r\n",
      "NDCG@10:  0.714900\r\n",
      "NDCG@20:  0.712100\r\n",
      "NDCG@100: 0.740300\r\n",
      "P20:      0.151700\r\n",
      "MAP:      0.606000\r\n",
      "MRR:      0.945300\r\n",
      "Recall:   0.577034\r\n"
     ]
    }
   ],
   "source": [
    "!cat collections/wikipedia_dpr_nq_sample/results/dev/final_exper/bm25_ance_knn/rep/out_100.rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating mixed dense-sparse (BM25+ANCE) embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note, we expect this mode is going primarily useful for purely sparse mixes. Otherwise, its efficiency is not great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data (currently we have no convenience wrapper scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p collections/wikipedia_dpr_nq_sample/derived_data/nmslib/bm25_ance_interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.resources.ResourceManager - Resource manager initialization. Resource root:\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.MapDbBackend - MapDB opened for reading: collections/wikipedia_dpr_nq_sample/forward_index/text.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.ForwardIndexBinaryDataDict - Finished loading context from file: collections/wikipedia_dpr_nq_sample/forward_index/text.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.MapDbBackend - MapDB opened for reading: collections/wikipedia_dpr_nq_sample/forward_index/dense.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.ForwardIndexBinaryDataDict - Finished loading context from file: collections/wikipedia_dpr_nq_sample/forward_index/dense.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.FeatExtrDenseDocEmbedDotProdSimilarity - Index field name: dense normalize embeddings:? false\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBSparse - Writing the number of entries (774392) to the output file\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBSparse - Exported 100000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBSparse - Exported 200000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBSparse - Exported 300000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBSparse - Exported 400000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBSparse - Exported 500000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBSparse - Exported 600000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBSparse - Exported 700000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBSparse - Exported 774392 docs\n"
     ]
    }
   ],
   "source": [
    "!target/appassembler/bin/ExportToNMSLIBSparse  \\\n",
    "    -extr_json collections/wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_ance.json \\\n",
    "    -model_file collections/wikipedia_dpr_nq_sample/exper_desc.best/models/bm25_ance.model \\\n",
    "    -fwd_index_dir collections/wikipedia_dpr_nq_sample/forward_index/ \\\n",
    "    -out_file collections/wikipedia_dpr_nq_sample/derived_data/nmslib/bm25_ance_interleaved/export.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compile the latest NMSLIB [query server (details)](https://github.com/nmslib/nmslib/blob/master/manual/query_server.md). Then it needs to be started from the FlexNeuART source tree root as shown below. In our example, the server is going to carry out the brute-force search. It is also possible to create an index using, e.g., HNSW. The __difference__ from the previous examples is that the similarity function here is a simple unweighted inner product between sparse vectors:\n",
    "\n",
    "```\n",
    "<path to the query server>/query_server \\\n",
    "    -p 8000 \\\n",
    "     -s negdotprod_sparse_bin_fast \\\n",
    "     -i collections/wikipedia_dpr_nq_sample/derived_data/nmslib/bm25_ance_interleaved/export.data \\\n",
    "     -m brute_force\n",
    "```\n",
    "\n",
    "Now we can run a benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: collections\n",
      "The number of CPU cores:      8\n",
      "The number of || experiments: 1\n",
      "The number of threads:        4\n",
      "================================================================================\n",
      "Experiment descriptor file:                                 collections/wikipedia_dpr_nq_sample/exper_desc.best/nmslib_bm25_ance_interleaved.json\n",
      "Default test set:                                           dev\n",
      "Number of parallel experiments:                             1\n",
      "Number of threads in feature extractors/query applications: 4\n",
      "================================================================================\n",
      "Parsed experiment parameters:\n",
      "experSubdir:final_exper/bm25_ance_knn_interleaved\n",
      "candProv:nmslib\n",
      "candProvAddConf:exper_desc.best/nmslib/bm25_ance_interleaved/cand_prov.json\n",
      "candProvURI:localhost:8000\n",
      "candQty:1000\n",
      "testOnly:1\n",
      "========================================\n",
      "Started a process 3054, working dir: collections/wikipedia_dpr_nq_sample/results/dev/final_exper/bm25_ance_knn_interleaved\n",
      "Process log file: collections/wikipedia_dpr_nq_sample/results/dev/final_exper/bm25_ance_knn_interleaved/exper.log\n",
      "Waiting for 1 child processes\n",
      "Process with pid=3054 finished successfully.\n",
      "Waiting for 0 child processes\n",
      "================================================================================\n",
      "1 experiments executed\n",
      "0 experiments failed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!scripts/exper/run_experiments.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    exper_desc.best/nmslib_bm25_ance_interleaved.json \\\n",
    "    -test_part dev \\\n",
    "    -thread_qty 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top 100 report__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of queries:    2500\r\n",
      "NDCG@10:  0.714900\r\n",
      "NDCG@20:  0.712100\r\n",
      "NDCG@100: 0.740300\r\n",
      "P20:      0.151700\r\n",
      "MAP:      0.606000\r\n",
      "MRR:      0.945300\r\n",
      "Recall:   0.577034\r\n"
     ]
    }
   ],
   "source": [
    "!cat collections/wikipedia_dpr_nq_sample/results/dev/final_exper/bm25_ance_knn/rep/out_100.rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating mixed dense-sparse (BM25+glove averaged) embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data (currently we have no convenience wrapper scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p collections/wikipedia_dpr_nq_sample/derived_data/nmslib/bm25_avgembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.resources.ResourceManager - Resource manager initialization. Resource root:\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.MapDbBackend - MapDB opened for reading: collections/wikipedia_dpr_nq_sample/forward_index/text.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.ForwardIndexBinaryDataDict - Finished loading context from file: collections/wikipedia_dpr_nq_sample/forward_index/text.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.MapDbBackend - MapDB opened for reading: collections/wikipedia_dpr_nq_sample/forward_index/text_unlemm.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.fwdindx.ForwardIndexBinaryDataDict - Finished loading context from file: collections/wikipedia_dpr_nq_sample/forward_index/text_unlemm.mapdb_dataDict\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.EmbeddingReaderAndRecoder - Loaded 50000 source word vectors from 'collections/wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2'\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.EmbeddingReaderAndRecoder - Loaded 100000 source word vectors from 'collections/wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2'\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.EmbeddingReaderAndRecoder - Loaded 150000 source word vectors from 'collections/wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2'\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.EmbeddingReaderAndRecoder - Loaded 200000 source word vectors from 'collections/wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2'\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.EmbeddingReaderAndRecoder - Loaded 250000 source word vectors from 'collections/wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2'\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.EmbeddingReaderAndRecoder - Loaded 300000 source word vectors from 'collections/wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2'\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.EmbeddingReaderAndRecoder - Loaded 350000 source word vectors from 'collections/wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2'\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.EmbeddingReaderAndRecoder - Loaded 400000 source word vectors from 'collections/wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2'\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.letor.EmbeddingReaderAndRecoder - Finished loading 271158 word vectors from 'collections/wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2' (out of 400000), dimensionality: 50\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Writing the number of entries (774392) to the output file\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 100000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 200000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 300000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 400000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 500000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 600000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 700000 docs\n",
      "[main] INFO edu.cmu.lti.oaqa.flexneuart.apps.ExportToNMSLIBDenseSparseFusion - Exported 774392 docs\n"
     ]
    }
   ],
   "source": [
    "!target/appassembler/bin/ExportToNMSLIBDenseSparseFusion  \\\n",
    "    -extr_json collections/wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_avgembed.json \\\n",
    "    -fwd_index_dir collections/wikipedia_dpr_nq_sample/forward_index/ \\\n",
    "    -embed_dir collections/wikipedia_dpr_nq_sample/derived_data/embeddings \\\n",
    "    -out_file collections/wikipedia_dpr_nq_sample/derived_data/nmslib/bm25_avgembed/export.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compile the latest NMSLIB [query server (details)](https://github.com/nmslib/nmslib/blob/master/manual/query_server.md). Then it needs to be started from the FlexNeuART source tree root as shown below. In our example, the server is going to carry out the brute-force search. It is also possible to create an index using, e.g., HNSW. The __difference__ from the previous examples is that the similarity function here is a simple unweighted inner product between sparse vectors:\n",
    "\n",
    "```\n",
    "<path to the query server>/query_server \\\n",
    "    -p 8000 \\\n",
    "     -s sparse_dense_fusion:weightfilename=\\\n",
    "collections/wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_avgembed/fusion_weights \\\n",
    "     -i collections/wikipedia_dpr_nq_sample/derived_data/nmslib/bm25_avgembed/export.data \\\n",
    "     -m brute_force\n",
    "```\n",
    "\n",
    "Now we can run a benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: collections\n",
      "The number of CPU cores:      8\n",
      "The number of || experiments: 1\n",
      "The number of threads:        4\n",
      "================================================================================\n",
      "Experiment descriptor file:                                 collections/wikipedia_dpr_nq_sample/exper_desc.best/nmslib_bm25_avgembed.json\n",
      "Default test set:                                           dev\n",
      "Number of parallel experiments:                             1\n",
      "Number of threads in feature extractors/query applications: 4\n",
      "================================================================================\n",
      "Parsed experiment parameters:\n",
      "experSubdir:final_exper/bm25_avgembed_knn\n",
      "candProv:nmslib\n",
      "candProvAddConf:exper_desc.best/nmslib/bm25_avgembed/cand_prov.json\n",
      "candProvURI:localhost:8000\n",
      "candQty:1000\n",
      "testOnly:1\n",
      "========================================\n",
      "Started a process 3267, working dir: collections/wikipedia_dpr_nq_sample/results/dev/final_exper/bm25_avgembed_knn\n",
      "Process log file: collections/wikipedia_dpr_nq_sample/results/dev/final_exper/bm25_avgembed_knn/exper.log\n",
      "Waiting for 1 child processes\n",
      "Process with pid=3267 finished successfully.\n",
      "Waiting for 0 child processes\n",
      "================================================================================\n",
      "1 experiments executed\n",
      "0 experiments failed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "!scripts/exper/run_experiments.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    exper_desc.best/nmslib_bm25_avgembed.json \\\n",
    "    -test_part dev \\\n",
    "    -thread_qty 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top 100 report__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of queries:    2500\r\n",
      "NDCG@10:  0.402300\r\n",
      "NDCG@20:  0.434900\r\n",
      "NDCG@100: 0.507800\r\n",
      "P20:      0.165600\r\n",
      "MAP:      0.345600\r\n",
      "MRR:      0.486600\r\n",
      "Recall:   0.820033\r\n"
     ]
    }
   ],
   "source": [
    "!cat collections/wikipedia_dpr_nq_sample/results/dev/final_exper/bm25_avgembed_knn/rep/out_100.rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
